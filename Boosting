# For use with Retention/Churn Exercise

library(lubridate)
library(dplyr)
library(psych)
library(BBmisc)# nice normalize function
library(purrr)
library(C50)
library(psych)
library(neuralnet)
library(dplyr)
library(pdp)
library(gmodels)
library(randomForest)
library(caret)
library(lubridate)
library(pacman)
library(MASS)
library(ROCR)
library(lift)
library(stringr)
library(nnet)
library(mltools)
library(data.table)


#Read in data file ** put in your path **

Transact.df<- read.csv("ISMSDataset1.csv")
# Fix dates - Create year variable

Transact.df$TRANSACTION_DATE_2<-ymd(dmy_hms(Transact.df$TRANSACTION_DATE))
Transact.df$TRANSACTION_DATE_YR<-year(Transact.df$TRANSACTION_DATE_2)

#Aggregate to household level, create variables you want
# NB: These are just example. Lots of opportunity for feature
# engineering here.

# NOTE: This will throw a warning, but that's OK ... 

hh_transact <- subset(Transact.df,TRANSACTION_DATE_YR<=2004) %>% group_by(HOUSEHOLD_ID) %>% 
  summarise(firstpurch = min(as.Date(TRANSACTION_DATE_2)),
            lastpurch02 = max(as.Date(TRANSACTION_DATE_2[TRANSACTION_DATE_YR<=2002])), #most recent purchase as of 12-31-2002
            purch98 = length(TRANSACTION_NBR[TRANSACTION_DATE_YR==1998]),
            purch99 = length(TRANSACTION_NBR[TRANSACTION_DATE_YR==1999]),
            purch00 = length(TRANSACTION_NBR[TRANSACTION_DATE_YR==2000]),
            purch01 = length(TRANSACTION_NBR[TRANSACTION_DATE_YR==2001]),
            purch02 = length(TRANSACTION_NBR[TRANSACTION_DATE_YR==2002]),
            purch03 = length(TRANSACTION_NBR[TRANSACTION_DATE_YR==2003]),
            purch04 = length(TRANSACTION_NBR[TRANSACTION_DATE_YR==2004]),
            doll98 = sum(EXTENDED_PRICE[TRANSACTION_DATE_YR==1998]),
            doll99 = sum(EXTENDED_PRICE[TRANSACTION_DATE_YR==1999]),
            doll00 = sum(EXTENDED_PRICE[TRANSACTION_DATE_YR==2000]),
            doll01 = sum(EXTENDED_PRICE[TRANSACTION_DATE_YR==2001]),
            doll02 = sum(EXTENDED_PRICE[TRANSACTION_DATE_YR==2002]),
            AGE_H_HEAD = first(AGE_H_HEAD),
            CHILDERN_PRESENCE = first(CHILDERN_PRESENCE),
            INCOME = first(INCOME),
            GENDER_H_HEAD = first(GENDER_H_HEAD),
            GENDER_INDIVIDUAL = first(GENDER_INDIVIDUAL),
            MALE_CHID_AGE_0_2 = first(MALE_CHID_AGE_0_2),
            MALE_CHID_AGE_3_5 = first(MALE_CHID_AGE_3_5),
            MALE_CHID_AGE_6_10 = first(MALE_CHID_AGE_6_10),
            MALE_CHID_AGE_11_15 = first(MALE_CHID_AGE_11_15),
            MALE_CHID_AGE_16_17 = first(MALE_CHID_AGE_16_17),
            FEMALE_CHID_AGE_0_2 = first(FEMALE_CHID_AGE_0_2),
            FEMALE_CHID_AGE_3_5 = first(FEMALE_CHID_AGE_3_5),
            FEMALE_CHID_AGE_6_10 = first(FEMALE_CHID_AGE_6_10),        
            FEMALE_CHID_AGE_11_15 = first(FEMALE_CHID_AGE_11_15),
            FEMALE_CHID_AGE_16_17 = first(FEMALE_CHID_AGE_16_17),       
            UNKNOWN_CHID_AGE_0_2 = first(UNKNOWN_CHID_AGE_0_2),
            UNKNOWN_CHID_AGE_3_5 = first(UNKNOWN_CHID_AGE_3_5),        
            UNKNOWN_CHID_AGE_6_10 = first(UNKNOWN_CHID_AGE_6_10),
            UNKNOWN_CHID_AGE_11_15 = first(UNKNOWN_CHID_AGE_11_15),      
            UNKNOWN_CHID_AGE_16_17 = first(UNKNOWN_CHID_AGE_16_17),
            numcat = n_distinct(CATEGORY_DESCRIPTION[TRANSACTION_DATE_YR>=1998 & TRANSACTION_DATE_YR<=2002]),
            numbrand = n_distinct(TRANSACTION_TYPE_DESCRIPTION[TRANSACTION_DATE_YR>=1998 & TRANSACTION_DATE_YR<=2002])
  )

# You will still need to deal with missings
#  For numerics, maybe just use means; for factors maybe "UNKNOWN"

# Below is one approach. There are many others.
# means for income and age

hh_transact$AGE_H_HEAD <- ifelse(is.na(hh_transact$AGE_H_HEAD), mean(hh_transact$AGE_H_HEAD,na.rm=TRUE),hh_transact$AGE_H_HEAD)
hh_transact$INCOME <- ifelse(is.na(hh_transact$INCOME), mean(hh_transact$INCOME,na.rm=TRUE),hh_transact$INCOME)

# New category for variables

hh_transact$CHILDERN_PRESENCE <- as.factor(ifelse(hh_transact$CHILDERN_PRESENCE=="N" | hh_transact$CHILDERN_PRESENCE=="Y",hh_transact$CHILDERN_PRESENCE, "UNKNOWN"))
hh_transact$GENDER_H_HEAD <- as.factor(hh_transact$GENDER_H_HEAD)
hh_transact$GENDER_INDIVIDUAL <- as.factor(ifelse(hh_transact$GENDER_INDIVIDUAL=="F" | hh_transact$GENDER_INDIVIDUAL=="M" ,hh_transact$GENDER_INDIVIDUAL, "UNKNOWN"))


# We want to get rid of those households that had their first purchase after 2002

hh_transact <- hh_transact[year(hh_transact$firstpurch)<=2002,]


# These will be useful in RFM calc

hh_transact$numpurch <- hh_transact$purch98 + hh_transact$purch99 + hh_transact$purch00 + hh_transact$purch01 + hh_transact$purch02
hh_transact$dollpurch <- hh_transact$doll98 + hh_transact$doll99 + hh_transact$doll00 + hh_transact$doll01 + hh_transact$doll02
hh_transact$recent <- as.numeric(as.Date("2002-12-31")-as.Date(hh_transact$lastpurch02))

# The two basic "retention" measures: at least on purchase in 2003 ...

hh_transact$ret03 <- (hh_transact$purch03>0)


# ... at least one purchase in 2004

hh_transact$ret04 <- (hh_transact$purch04>0)

#############################################################################
# RFM measures as of end of 2002

# Recency Measure is most recent purchase
# Frequency is purchase rate since acquired (purchases per )
# Monetary is dollar purchase rate

# Average purchase frequency from first purchase until end of 2002:

hh_transact$frequent <- hh_transact$numpurch/(as.numeric(as.Date("2002-12-31")- hh_transact$firstpurch))*30

# Average purchase amount from first purchase until end of 2002:

hh_transact$monetary <- hh_transact$dollpurch/(as.numeric(as.Date("2002-12-31")- hh_transact$firstpurch))*30

# Now put into "quintiles" - there are 19474 hh's, so lets do 4000 - 4000 - 4000 - 4000 - 3474

hh_transact <- as.data.frame(hh_transact[order(hh_transact$recent),])
hh_transact$rix <- 1:nrow(hh_transact) # recency index
hh_transact$R_Q<-ceiling(hh_transact$rix/4000)

hh_transact <- as.data.frame(hh_transact[order(-hh_transact$frequent),])
hh_transact$fix <- 1:nrow(hh_transact) # frequency index
hh_transact$F_Q<-ceiling(hh_transact$fix/4000)

hh_transact <- as.data.frame(hh_transact[order(-hh_transact$monetary),])
hh_transact$mix <- 1:nrow(hh_transact) # monetary index
hh_transact$M_Q<-ceiling(hh_transact$mix/4000)

# Create one RFM score

hh_transact$RFMscore <- hh_transact$R_Q*100+hh_transact$F_Q*10+hh_transact$M_Q



#transfer to factor
hh_transact$ret03<-ifelse(hh_transact$ret03==TRUE,1,0)
hh_transact$ret04<-ifelse(hh_transact$ret04==TRUE,1,0)
hh_transact$ret03 <-as.factor(hh_transact$ret03)
hh_transact$ret04 <-as.factor(hh_transact$ret04)


colnames(hh_transact)

#Create Validation sample/training and testing sample


set.seed(13343)
train_ind <- createDataPartition(y = hh_transact$ret03,p=.8,list = FALSE)
training <- hh_transact[train_ind,]
test <- hh_transact[-train_ind,]

prop.table(table(transac_sample$ret03))
prop.table(table(training$ret03))
prop.table(table(test$ret03))


set.seed(34331)
val_ind <- createDataPartition(y = hh_transact$ret03,p=.25,list = FALSE)
val <- training[val_ind,]
training <- training[-val_ind,]

# RUN PLAIN VANILLA DECISION TREE
colnames(training)

nondates<-c(4:8,11:40,43:44,46,48,50:51)

dtmodel <- C5.0(training[,nondates],training$ret03) #Accuracy:0.7711 #Kappa:0.2341 #Sensitivity:0.24174  
summary(dtmodel)
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", val$ret03)


# ADAPTIVE BOOSTING
dtmodel <- C5.0(training[,nondates],training$ret03, trials=4) #Accuracy:0.7693 #Kappa:0.217 #Sensitivity:0.22107
summary(dtmodel)
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", val$ret03)

dtmodel <- C5.0(training[,nondates],training$ret03, trials=5) #Accuracy:0.7663 #Kappa:0.2302 #Sensitivity:0.25103
summary(dtmodel)
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", val$ret03)

dtmodel <- C5.0(training[,nondates],training$ret03, trials=6) #Accuracy:0.7717 #Kappa:0.2176 #Sensitivity:0.21488
summary(dtmodel)
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", val$ret03)


################ 
# Oversampling #
################ 

# Here we augment the training data by adding more observations in the target
#  class ("1"). We make them 50/50.


pos_needed <- nrow(training[training$ret03==0,])  # at 50/50 we need as many pos as neg
pos_corpus <- training[training$ret03==1,]
neg_corpus <- training[training$ret03==0,]


#create vector of pos_needed integers to sample the positives
# NOTE: this is sampling WITH replacement

set.seed(12345)
drawvec <- floor(runif(pos_needed, min=1, nrow(pos_corpus)))

# randomly draw from pos corpus and add to to negative to form new
#  training set

for (i in 1:pos_needed){
  neg_corpus <- rbind(neg_corpus,pos_corpus[drawvec[i],])
} 

# "os" = oversampling

os_training <- neg_corpus

colnames(os_training)
#weight:10 #trials:4
#Accuracy:0.7009 #Kappa:0.1997 #Sensitivity:0.39979
dtmodel <- C5.0(os_training[,nondates],as.factor(os_training[,41]),weights=c(1,10),trials=4) 
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", as.factor(val$ret03))

#weight:50 #trials:4
#Accuracy:0.6873 #Kappa:0.1642 #Sensitivity:0.37397
dtmodel <- C5.0(os_training[,nondates],as.factor(os_training[,41]),weights=c(1,50),trials=4) 
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", as.factor(val$ret03))

#weight:100 #trials:4
#Accuracy:0.6799 #Kappa:0.171 #Sensitivity:0.4070
dtmodel <- C5.0(os_training[,nondates],as.factor(os_training[,41]),weights=c(1,100),trials=4) 
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", as.factor(val$ret03))

###BEST ONE AFTER TUNNING & OVERSAMPLING###
#weight:10 #trials:5
#Accuracy:0.6717 #Kappa:0.1799 #Sensitivity:0.4473
dtmodel_b <- C5.0(os_training[,nondates],as.factor(os_training[,41]),weights=c(1,10),trials=5) 
dt_class <- predict(dtmodel_b,val[,nondates])
confusionMatrix(dt_class, positive = "1", as.factor(val$ret03))

#weight:50 #trials:5
#Accuracy:0.6948 #Kappa:0.1855 #Sensitivity:0.39153
dtmodel <- C5.0(os_training[,nondates],as.factor(os_training[,41]),weights=c(1,50),trials=5) 
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", as.factor(val$ret03))

#weight:100 #trials:5
#Accuracy:0.695 #Kappa:0.203 #Sensitivity:0.4236
dtmodel <- C5.0(os_training[,nondates],as.factor(os_training[,41]),weights=c(1,100),trials=5) 
dt_class <- predict(dtmodel,val[,nondates])
confusionMatrix(dt_class, positive = "1", as.factor(val$ret03))


#predict2003
#Accuracy:0.6772 #Kappa:0.1934 #Sensitivity:0.4595
dt_class_03<- predict(dtmodel_b,test[,nondates])
confusionMatrix(dt_class_03, positive = "1", as.factor(test$ret03)) 

#Accuracy:0.68 #Kappa:0.1567 #Sensitivity:0.4557
dt_class_04<- predict(dtmodel_b,test[,nondates])
confusionMatrix(dt_class_04, positive = "1", as.factor(test$ret04)) 
